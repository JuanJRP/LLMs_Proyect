{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9984,
  "eval_steps": 500,
  "global_step": 1404,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.4570862054824829,
      "learning_rate": 1.8e-05,
      "loss": 3.1394,
      "step": 10
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.6242455840110779,
      "learning_rate": 3.8e-05,
      "loss": 3.0785,
      "step": 20
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.7982631921768188,
      "learning_rate": 5.8e-05,
      "loss": 3.0424,
      "step": 30
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.6874598264694214,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.876,
      "step": 40
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.8677590489387512,
      "learning_rate": 9.8e-05,
      "loss": 2.776,
      "step": 50
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.2642074823379517,
      "learning_rate": 0.000118,
      "loss": 2.7073,
      "step": 60
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.6708933711051941,
      "learning_rate": 0.000138,
      "loss": 2.6199,
      "step": 70
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.8581869006156921,
      "learning_rate": 0.00015800000000000002,
      "loss": 2.5792,
      "step": 80
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.5365809798240662,
      "learning_rate": 0.00017800000000000002,
      "loss": 2.5751,
      "step": 90
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.572228193283081,
      "learning_rate": 0.00019800000000000002,
      "loss": 2.5222,
      "step": 100
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.5109777450561523,
      "learning_rate": 0.0001986196319018405,
      "loss": 2.4521,
      "step": 110
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.5376030802726746,
      "learning_rate": 0.00019708588957055215,
      "loss": 2.4999,
      "step": 120
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.556172788143158,
      "learning_rate": 0.00019555214723926382,
      "loss": 2.5299,
      "step": 130
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.5517964959144592,
      "learning_rate": 0.00019401840490797546,
      "loss": 2.4731,
      "step": 140
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.5147361755371094,
      "learning_rate": 0.00019248466257668713,
      "loss": 2.4823,
      "step": 150
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.5089366436004639,
      "learning_rate": 0.00019095092024539877,
      "loss": 2.5248,
      "step": 160
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.6514060497283936,
      "learning_rate": 0.00018941717791411044,
      "loss": 2.4773,
      "step": 170
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.5549396872520447,
      "learning_rate": 0.00018788343558282208,
      "loss": 2.4419,
      "step": 180
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.537999153137207,
      "learning_rate": 0.00018634969325153375,
      "loss": 2.4052,
      "step": 190
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.5690499544143677,
      "learning_rate": 0.00018481595092024542,
      "loss": 2.4496,
      "step": 200
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.5854473114013672,
      "learning_rate": 0.00018328220858895706,
      "loss": 2.3845,
      "step": 210
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.5543099045753479,
      "learning_rate": 0.00018174846625766873,
      "loss": 2.4518,
      "step": 220
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.5273390412330627,
      "learning_rate": 0.00018021472392638037,
      "loss": 2.4709,
      "step": 230
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.5209898948669434,
      "learning_rate": 0.00017868098159509204,
      "loss": 2.3986,
      "step": 240
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.56431645154953,
      "learning_rate": 0.0001771472392638037,
      "loss": 2.4172,
      "step": 250
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.5768306851387024,
      "learning_rate": 0.00017561349693251535,
      "loss": 2.4142,
      "step": 260
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.5509131550788879,
      "learning_rate": 0.00017407975460122702,
      "loss": 2.4725,
      "step": 270
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.5900889039039612,
      "learning_rate": 0.00017254601226993866,
      "loss": 2.4808,
      "step": 280
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.5341798663139343,
      "learning_rate": 0.00017101226993865033,
      "loss": 2.3847,
      "step": 290
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.5358065366744995,
      "learning_rate": 0.000169478527607362,
      "loss": 2.4727,
      "step": 300
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.5480138063430786,
      "learning_rate": 0.00016794478527607364,
      "loss": 2.4349,
      "step": 310
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.5722423791885376,
      "learning_rate": 0.00016641104294478528,
      "loss": 2.4638,
      "step": 320
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.5455640554428101,
      "learning_rate": 0.00016487730061349695,
      "loss": 2.3851,
      "step": 330
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.5783746838569641,
      "learning_rate": 0.0001633435582822086,
      "loss": 2.4456,
      "step": 340
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.6180315017700195,
      "learning_rate": 0.00016180981595092026,
      "loss": 2.4613,
      "step": 350
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.6226357817649841,
      "learning_rate": 0.0001602760736196319,
      "loss": 2.4384,
      "step": 360
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.6390542387962341,
      "learning_rate": 0.00015874233128834357,
      "loss": 2.4321,
      "step": 370
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.6428000330924988,
      "learning_rate": 0.0001572085889570552,
      "loss": 2.4304,
      "step": 380
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.5391844511032104,
      "learning_rate": 0.00015567484662576688,
      "loss": 2.3839,
      "step": 390
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.5780192613601685,
      "learning_rate": 0.00015414110429447854,
      "loss": 2.4702,
      "step": 400
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.6852620244026184,
      "learning_rate": 0.00015260736196319019,
      "loss": 2.3864,
      "step": 410
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.5406144857406616,
      "learning_rate": 0.00015107361963190185,
      "loss": 2.4063,
      "step": 420
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.5937915444374084,
      "learning_rate": 0.0001495398773006135,
      "loss": 2.4197,
      "step": 430
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.5810943245887756,
      "learning_rate": 0.00014800613496932516,
      "loss": 2.4491,
      "step": 440
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.5730466246604919,
      "learning_rate": 0.00014647239263803683,
      "loss": 2.3897,
      "step": 450
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.58671635389328,
      "learning_rate": 0.00014493865030674847,
      "loss": 2.4513,
      "step": 460
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.5954777002334595,
      "learning_rate": 0.00014340490797546014,
      "loss": 2.5654,
      "step": 470
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.6339678168296814,
      "learning_rate": 0.00014187116564417178,
      "loss": 2.3626,
      "step": 480
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.6588530540466309,
      "learning_rate": 0.00014033742331288345,
      "loss": 2.3977,
      "step": 490
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.6676056981086731,
      "learning_rate": 0.00013880368098159512,
      "loss": 2.3756,
      "step": 500
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.6387247443199158,
      "learning_rate": 0.00013726993865030676,
      "loss": 2.3325,
      "step": 510
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.6744348406791687,
      "learning_rate": 0.0001357361963190184,
      "loss": 2.3472,
      "step": 520
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.6326634883880615,
      "learning_rate": 0.00013420245398773005,
      "loss": 2.3589,
      "step": 530
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.6840425133705139,
      "learning_rate": 0.00013266871165644171,
      "loss": 2.4233,
      "step": 540
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.6939998865127563,
      "learning_rate": 0.00013113496932515338,
      "loss": 2.3396,
      "step": 550
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.7292037606239319,
      "learning_rate": 0.00012960122699386502,
      "loss": 2.4312,
      "step": 560
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.7009084820747375,
      "learning_rate": 0.0001280674846625767,
      "loss": 2.3882,
      "step": 570
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.6426689624786377,
      "learning_rate": 0.00012653374233128833,
      "loss": 2.3765,
      "step": 580
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.6551823616027832,
      "learning_rate": 0.000125,
      "loss": 2.369,
      "step": 590
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.684775173664093,
      "learning_rate": 0.00012346625766871167,
      "loss": 2.4067,
      "step": 600
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.6127550601959229,
      "learning_rate": 0.00012193251533742331,
      "loss": 2.468,
      "step": 610
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.6685969829559326,
      "learning_rate": 0.00012039877300613498,
      "loss": 2.3714,
      "step": 620
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.7122732996940613,
      "learning_rate": 0.00011886503067484662,
      "loss": 2.3795,
      "step": 630
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.6579571962356567,
      "learning_rate": 0.00011733128834355829,
      "loss": 2.3508,
      "step": 640
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.6941933631896973,
      "learning_rate": 0.00011579754601226996,
      "loss": 2.3853,
      "step": 650
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.6681801080703735,
      "learning_rate": 0.0001142638036809816,
      "loss": 2.3587,
      "step": 660
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.6201761960983276,
      "learning_rate": 0.00011273006134969326,
      "loss": 2.3291,
      "step": 670
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.690030038356781,
      "learning_rate": 0.0001111963190184049,
      "loss": 2.4058,
      "step": 680
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.7483643293380737,
      "learning_rate": 0.00010966257668711657,
      "loss": 2.3922,
      "step": 690
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.7160443067550659,
      "learning_rate": 0.00010812883435582823,
      "loss": 2.3714,
      "step": 700
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.646649181842804,
      "learning_rate": 0.00010659509202453988,
      "loss": 2.3381,
      "step": 710
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.6556049585342407,
      "learning_rate": 0.00010506134969325154,
      "loss": 2.3356,
      "step": 720
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.632619321346283,
      "learning_rate": 0.00010352760736196319,
      "loss": 2.3428,
      "step": 730
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.7208462953567505,
      "learning_rate": 0.00010199386503067485,
      "loss": 2.3415,
      "step": 740
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.7310654520988464,
      "learning_rate": 0.00010046012269938651,
      "loss": 2.3677,
      "step": 750
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.7552948594093323,
      "learning_rate": 9.892638036809816e-05,
      "loss": 2.4031,
      "step": 760
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.7557883858680725,
      "learning_rate": 9.739263803680982e-05,
      "loss": 2.3854,
      "step": 770
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.7001786828041077,
      "learning_rate": 9.585889570552147e-05,
      "loss": 2.373,
      "step": 780
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.7664140462875366,
      "learning_rate": 9.432515337423313e-05,
      "loss": 2.3444,
      "step": 790
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.68214350938797,
      "learning_rate": 9.279141104294478e-05,
      "loss": 2.3452,
      "step": 800
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.6240657567977905,
      "learning_rate": 9.125766871165644e-05,
      "loss": 2.3927,
      "step": 810
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.7061984539031982,
      "learning_rate": 8.972392638036811e-05,
      "loss": 2.3698,
      "step": 820
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.7295087575912476,
      "learning_rate": 8.819018404907976e-05,
      "loss": 2.3693,
      "step": 830
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.7887717485427856,
      "learning_rate": 8.665644171779142e-05,
      "loss": 2.4086,
      "step": 840
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.6851483583450317,
      "learning_rate": 8.512269938650307e-05,
      "loss": 2.3878,
      "step": 850
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.6875462532043457,
      "learning_rate": 8.358895705521473e-05,
      "loss": 2.3483,
      "step": 860
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.7833808660507202,
      "learning_rate": 8.205521472392638e-05,
      "loss": 2.4131,
      "step": 870
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.6580358147621155,
      "learning_rate": 8.052147239263804e-05,
      "loss": 2.3785,
      "step": 880
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.6821941137313843,
      "learning_rate": 7.898773006134969e-05,
      "loss": 2.3262,
      "step": 890
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.7307356595993042,
      "learning_rate": 7.745398773006135e-05,
      "loss": 2.3543,
      "step": 900
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.69510817527771,
      "learning_rate": 7.5920245398773e-05,
      "loss": 2.3678,
      "step": 910
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.6901982426643372,
      "learning_rate": 7.438650306748467e-05,
      "loss": 2.3862,
      "step": 920
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.7477316856384277,
      "learning_rate": 7.285276073619633e-05,
      "loss": 2.3897,
      "step": 930
    },
    {
      "epoch": 2.0085333333333333,
      "grad_norm": 0.6537507772445679,
      "learning_rate": 7.131901840490798e-05,
      "loss": 2.5164,
      "step": 940
    },
    {
      "epoch": 2.0298666666666665,
      "grad_norm": 0.7332677245140076,
      "learning_rate": 6.978527607361964e-05,
      "loss": 2.3088,
      "step": 950
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.822270929813385,
      "learning_rate": 6.825153374233129e-05,
      "loss": 2.2865,
      "step": 960
    },
    {
      "epoch": 2.0725333333333333,
      "grad_norm": 0.7434226870536804,
      "learning_rate": 6.671779141104295e-05,
      "loss": 2.2934,
      "step": 970
    },
    {
      "epoch": 2.0938666666666665,
      "grad_norm": 0.7367339134216309,
      "learning_rate": 6.51840490797546e-05,
      "loss": 2.3167,
      "step": 980
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.7248939275741577,
      "learning_rate": 6.365030674846626e-05,
      "loss": 2.2574,
      "step": 990
    },
    {
      "epoch": 2.1365333333333334,
      "grad_norm": 0.7549430727958679,
      "learning_rate": 6.211656441717791e-05,
      "loss": 2.3668,
      "step": 1000
    },
    {
      "epoch": 2.1578666666666666,
      "grad_norm": 0.7970989346504211,
      "learning_rate": 6.058282208588958e-05,
      "loss": 2.3384,
      "step": 1010
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.8082028031349182,
      "learning_rate": 5.9049079754601235e-05,
      "loss": 2.3666,
      "step": 1020
    },
    {
      "epoch": 2.2005333333333335,
      "grad_norm": 0.7592953443527222,
      "learning_rate": 5.751533742331289e-05,
      "loss": 2.2921,
      "step": 1030
    },
    {
      "epoch": 2.2218666666666667,
      "grad_norm": 0.7836984395980835,
      "learning_rate": 5.598159509202454e-05,
      "loss": 2.2741,
      "step": 1040
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.8220634460449219,
      "learning_rate": 5.4447852760736193e-05,
      "loss": 2.3328,
      "step": 1050
    },
    {
      "epoch": 2.2645333333333335,
      "grad_norm": 0.7453336119651794,
      "learning_rate": 5.291411042944786e-05,
      "loss": 2.3576,
      "step": 1060
    },
    {
      "epoch": 2.2858666666666667,
      "grad_norm": 0.7479041814804077,
      "learning_rate": 5.138036809815952e-05,
      "loss": 2.315,
      "step": 1070
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.7859506011009216,
      "learning_rate": 4.984662576687117e-05,
      "loss": 2.2909,
      "step": 1080
    },
    {
      "epoch": 2.3285333333333336,
      "grad_norm": 0.767388641834259,
      "learning_rate": 4.831288343558282e-05,
      "loss": 2.3252,
      "step": 1090
    },
    {
      "epoch": 2.3498666666666668,
      "grad_norm": 0.8317002058029175,
      "learning_rate": 4.677914110429448e-05,
      "loss": 2.306,
      "step": 1100
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.7832031846046448,
      "learning_rate": 4.524539877300614e-05,
      "loss": 2.3006,
      "step": 1110
    },
    {
      "epoch": 2.392533333333333,
      "grad_norm": 0.7131213545799255,
      "learning_rate": 4.371165644171779e-05,
      "loss": 2.2667,
      "step": 1120
    },
    {
      "epoch": 2.413866666666667,
      "grad_norm": 0.8600342273712158,
      "learning_rate": 4.2177914110429454e-05,
      "loss": 2.3797,
      "step": 1130
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.7758011221885681,
      "learning_rate": 4.06441717791411e-05,
      "loss": 2.3402,
      "step": 1140
    },
    {
      "epoch": 2.4565333333333332,
      "grad_norm": 0.7872250080108643,
      "learning_rate": 3.9110429447852764e-05,
      "loss": 2.2961,
      "step": 1150
    },
    {
      "epoch": 2.4778666666666664,
      "grad_norm": 0.8189517259597778,
      "learning_rate": 3.757668711656442e-05,
      "loss": 2.3778,
      "step": 1160
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.8018539547920227,
      "learning_rate": 3.6042944785276074e-05,
      "loss": 2.2872,
      "step": 1170
    },
    {
      "epoch": 2.5205333333333333,
      "grad_norm": 0.9067864418029785,
      "learning_rate": 3.4509202453987735e-05,
      "loss": 2.2987,
      "step": 1180
    },
    {
      "epoch": 2.5418666666666665,
      "grad_norm": 0.7533835172653198,
      "learning_rate": 3.2975460122699384e-05,
      "loss": 2.3116,
      "step": 1190
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.8524506688117981,
      "learning_rate": 3.1441717791411045e-05,
      "loss": 2.3111,
      "step": 1200
    },
    {
      "epoch": 2.5845333333333333,
      "grad_norm": 0.9736555218696594,
      "learning_rate": 2.99079754601227e-05,
      "loss": 2.3639,
      "step": 1210
    },
    {
      "epoch": 2.6058666666666666,
      "grad_norm": 0.7769108414649963,
      "learning_rate": 2.837423312883436e-05,
      "loss": 2.3517,
      "step": 1220
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.7873964905738831,
      "learning_rate": 2.6840490797546014e-05,
      "loss": 2.2734,
      "step": 1230
    },
    {
      "epoch": 2.6485333333333334,
      "grad_norm": 0.8765695095062256,
      "learning_rate": 2.530674846625767e-05,
      "loss": 2.3482,
      "step": 1240
    },
    {
      "epoch": 2.6698666666666666,
      "grad_norm": 0.8366670608520508,
      "learning_rate": 2.3773006134969324e-05,
      "loss": 2.3222,
      "step": 1250
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.7865760326385498,
      "learning_rate": 2.2239263803680982e-05,
      "loss": 2.3697,
      "step": 1260
    },
    {
      "epoch": 2.7125333333333335,
      "grad_norm": 0.8655271530151367,
      "learning_rate": 2.0705521472392637e-05,
      "loss": 2.3075,
      "step": 1270
    },
    {
      "epoch": 2.7338666666666667,
      "grad_norm": 0.7545889616012573,
      "learning_rate": 1.9171779141104296e-05,
      "loss": 2.2741,
      "step": 1280
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.7680073976516724,
      "learning_rate": 1.7638036809815954e-05,
      "loss": 2.3024,
      "step": 1290
    },
    {
      "epoch": 2.776533333333333,
      "grad_norm": 0.7935988306999207,
      "learning_rate": 1.6104294478527606e-05,
      "loss": 2.3518,
      "step": 1300
    },
    {
      "epoch": 2.7978666666666667,
      "grad_norm": 0.8431664109230042,
      "learning_rate": 1.4570552147239264e-05,
      "loss": 2.3352,
      "step": 1310
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.8269655704498291,
      "learning_rate": 1.303680981595092e-05,
      "loss": 2.2895,
      "step": 1320
    },
    {
      "epoch": 2.840533333333333,
      "grad_norm": 0.8500573635101318,
      "learning_rate": 1.1503067484662577e-05,
      "loss": 2.3449,
      "step": 1330
    },
    {
      "epoch": 2.861866666666667,
      "grad_norm": 0.8448325991630554,
      "learning_rate": 9.969325153374232e-06,
      "loss": 2.3864,
      "step": 1340
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.9251354336738586,
      "learning_rate": 8.435582822085889e-06,
      "loss": 2.3866,
      "step": 1350
    },
    {
      "epoch": 2.904533333333333,
      "grad_norm": 0.7416691780090332,
      "learning_rate": 6.901840490797547e-06,
      "loss": 2.2948,
      "step": 1360
    },
    {
      "epoch": 2.925866666666667,
      "grad_norm": 0.9151337146759033,
      "learning_rate": 5.368098159509203e-06,
      "loss": 2.3158,
      "step": 1370
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.8632224798202515,
      "learning_rate": 3.834355828220859e-06,
      "loss": 2.324,
      "step": 1380
    },
    {
      "epoch": 2.9685333333333332,
      "grad_norm": 0.8287913203239441,
      "learning_rate": 2.3006134969325154e-06,
      "loss": 2.3149,
      "step": 1390
    },
    {
      "epoch": 2.989866666666667,
      "grad_norm": 0.9197462201118469,
      "learning_rate": 7.668711656441718e-07,
      "loss": 2.3184,
      "step": 1400
    }
  ],
  "logging_steps": 10,
  "max_steps": 1404,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2825543749599232e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
