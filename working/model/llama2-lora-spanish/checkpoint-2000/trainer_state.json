{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.494266323426164,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012481472813792027,
      "grad_norm": 0.9015370011329651,
      "learning_rate": 1.8e-05,
      "loss": 3.8433,
      "step": 10
    },
    {
      "epoch": 0.024962945627584054,
      "grad_norm": 1.3219751119613647,
      "learning_rate": 3.8e-05,
      "loss": 3.73,
      "step": 20
    },
    {
      "epoch": 0.03744441844137608,
      "grad_norm": 2.072570562362671,
      "learning_rate": 5.8e-05,
      "loss": 3.3935,
      "step": 30
    },
    {
      "epoch": 0.04992589125516811,
      "grad_norm": 1.666624665260315,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.8399,
      "step": 40
    },
    {
      "epoch": 0.062407364068960136,
      "grad_norm": 1.0967555046081543,
      "learning_rate": 9.8e-05,
      "loss": 2.2873,
      "step": 50
    },
    {
      "epoch": 0.07488883688275216,
      "grad_norm": 1.003645658493042,
      "learning_rate": 0.000118,
      "loss": 1.9417,
      "step": 60
    },
    {
      "epoch": 0.08737030969654419,
      "grad_norm": 2.4260549545288086,
      "learning_rate": 0.000138,
      "loss": 1.647,
      "step": 70
    },
    {
      "epoch": 0.09985178251033622,
      "grad_norm": 0.8722559213638306,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.4066,
      "step": 80
    },
    {
      "epoch": 0.11233325532412824,
      "grad_norm": 0.8812877535820007,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.3584,
      "step": 90
    },
    {
      "epoch": 0.12481472813792027,
      "grad_norm": 0.7935503721237183,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.2636,
      "step": 100
    },
    {
      "epoch": 0.1372962009517123,
      "grad_norm": 1.02008056640625,
      "learning_rate": 0.00019921841076856275,
      "loss": 1.2285,
      "step": 110
    },
    {
      "epoch": 0.14977767376550433,
      "grad_norm": 0.8592827916145325,
      "learning_rate": 0.00019834997828918803,
      "loss": 1.2009,
      "step": 120
    },
    {
      "epoch": 0.16225914657929635,
      "grad_norm": 0.8846461772918701,
      "learning_rate": 0.0001974815458098133,
      "loss": 1.1779,
      "step": 130
    },
    {
      "epoch": 0.17474061939308838,
      "grad_norm": 0.8810585141181946,
      "learning_rate": 0.00019661311333043856,
      "loss": 1.12,
      "step": 140
    },
    {
      "epoch": 0.1872220922068804,
      "grad_norm": 0.8970733880996704,
      "learning_rate": 0.00019574468085106384,
      "loss": 1.0985,
      "step": 150
    },
    {
      "epoch": 0.19970356502067244,
      "grad_norm": 0.9935007095336914,
      "learning_rate": 0.00019487624837168912,
      "loss": 1.0574,
      "step": 160
    },
    {
      "epoch": 0.21218503783446446,
      "grad_norm": 0.9685680866241455,
      "learning_rate": 0.00019400781589231438,
      "loss": 1.1028,
      "step": 170
    },
    {
      "epoch": 0.2246665106482565,
      "grad_norm": 1.120434045791626,
      "learning_rate": 0.00019313938341293966,
      "loss": 1.0493,
      "step": 180
    },
    {
      "epoch": 0.23714798346204852,
      "grad_norm": 1.0176340341567993,
      "learning_rate": 0.00019227095093356494,
      "loss": 1.053,
      "step": 190
    },
    {
      "epoch": 0.24962945627584054,
      "grad_norm": 1.1540002822875977,
      "learning_rate": 0.0001914025184541902,
      "loss": 0.9975,
      "step": 200
    },
    {
      "epoch": 0.26211092908963257,
      "grad_norm": 0.9208385348320007,
      "learning_rate": 0.00019053408597481544,
      "loss": 1.0062,
      "step": 210
    },
    {
      "epoch": 0.2745924019034246,
      "grad_norm": 1.051064133644104,
      "learning_rate": 0.00018966565349544075,
      "loss": 0.9955,
      "step": 220
    },
    {
      "epoch": 0.2870738747172166,
      "grad_norm": 1.3033710718154907,
      "learning_rate": 0.000188797221016066,
      "loss": 0.9846,
      "step": 230
    },
    {
      "epoch": 0.29955534753100865,
      "grad_norm": 1.2393670082092285,
      "learning_rate": 0.00018792878853669129,
      "loss": 0.9667,
      "step": 240
    },
    {
      "epoch": 0.3120368203448007,
      "grad_norm": 1.0824238061904907,
      "learning_rate": 0.00018706035605731657,
      "loss": 0.971,
      "step": 250
    },
    {
      "epoch": 0.3245182931585927,
      "grad_norm": 1.1653387546539307,
      "learning_rate": 0.00018619192357794182,
      "loss": 0.955,
      "step": 260
    },
    {
      "epoch": 0.33699976597238473,
      "grad_norm": 1.059927225112915,
      "learning_rate": 0.0001853234910985671,
      "loss": 0.9416,
      "step": 270
    },
    {
      "epoch": 0.34948123878617676,
      "grad_norm": 1.0101571083068848,
      "learning_rate": 0.00018445505861919235,
      "loss": 0.9596,
      "step": 280
    },
    {
      "epoch": 0.3619627115999688,
      "grad_norm": 1.1652154922485352,
      "learning_rate": 0.00018358662613981763,
      "loss": 0.9589,
      "step": 290
    },
    {
      "epoch": 0.3744441844137608,
      "grad_norm": 1.1164389848709106,
      "learning_rate": 0.00018271819366044291,
      "loss": 0.9411,
      "step": 300
    },
    {
      "epoch": 0.38692565722755284,
      "grad_norm": 1.2175557613372803,
      "learning_rate": 0.00018184976118106817,
      "loss": 0.9393,
      "step": 310
    },
    {
      "epoch": 0.39940713004134487,
      "grad_norm": 1.1531304121017456,
      "learning_rate": 0.00018098132870169345,
      "loss": 0.9306,
      "step": 320
    },
    {
      "epoch": 0.4118886028551369,
      "grad_norm": 1.3916641473770142,
      "learning_rate": 0.00018011289622231873,
      "loss": 0.924,
      "step": 330
    },
    {
      "epoch": 0.4243700756689289,
      "grad_norm": 1.4153062105178833,
      "learning_rate": 0.00017924446374294398,
      "loss": 0.9255,
      "step": 340
    },
    {
      "epoch": 0.43685154848272095,
      "grad_norm": 1.2403775453567505,
      "learning_rate": 0.0001783760312635693,
      "loss": 0.9224,
      "step": 350
    },
    {
      "epoch": 0.449333021296513,
      "grad_norm": 1.1314239501953125,
      "learning_rate": 0.00017750759878419454,
      "loss": 0.9203,
      "step": 360
    },
    {
      "epoch": 0.461814494110305,
      "grad_norm": 1.0694482326507568,
      "learning_rate": 0.0001766391663048198,
      "loss": 0.8969,
      "step": 370
    },
    {
      "epoch": 0.47429596692409703,
      "grad_norm": 1.088707447052002,
      "learning_rate": 0.00017577073382544508,
      "loss": 0.8911,
      "step": 380
    },
    {
      "epoch": 0.48677743973788906,
      "grad_norm": 1.164042353630066,
      "learning_rate": 0.00017490230134607036,
      "loss": 0.8827,
      "step": 390
    },
    {
      "epoch": 0.4992589125516811,
      "grad_norm": 1.2942827939987183,
      "learning_rate": 0.0001740338688666956,
      "loss": 0.8667,
      "step": 400
    },
    {
      "epoch": 0.5117403853654732,
      "grad_norm": 1.2377161979675293,
      "learning_rate": 0.0001731654363873209,
      "loss": 0.8806,
      "step": 410
    },
    {
      "epoch": 0.5242218581792651,
      "grad_norm": 1.5156304836273193,
      "learning_rate": 0.00017229700390794617,
      "loss": 0.887,
      "step": 420
    },
    {
      "epoch": 0.5367033309930572,
      "grad_norm": 1.211997389793396,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.8783,
      "step": 430
    },
    {
      "epoch": 0.5491848038068492,
      "grad_norm": 1.2582945823669434,
      "learning_rate": 0.0001705601389491967,
      "loss": 0.8576,
      "step": 440
    },
    {
      "epoch": 0.5616662766206413,
      "grad_norm": 1.277374505996704,
      "learning_rate": 0.00016969170646982199,
      "loss": 0.8537,
      "step": 450
    },
    {
      "epoch": 0.5741477494344333,
      "grad_norm": 1.1946576833724976,
      "learning_rate": 0.00016882327399044727,
      "loss": 0.8554,
      "step": 460
    },
    {
      "epoch": 0.5866292222482253,
      "grad_norm": 1.301284909248352,
      "learning_rate": 0.00016795484151107252,
      "loss": 0.8658,
      "step": 470
    },
    {
      "epoch": 0.5991106950620173,
      "grad_norm": 1.3264299631118774,
      "learning_rate": 0.0001670864090316978,
      "loss": 0.8357,
      "step": 480
    },
    {
      "epoch": 0.6115921678758094,
      "grad_norm": 1.243342399597168,
      "learning_rate": 0.00016621797655232308,
      "loss": 0.8422,
      "step": 490
    },
    {
      "epoch": 0.6240736406896014,
      "grad_norm": 1.2866705656051636,
      "learning_rate": 0.00016534954407294833,
      "loss": 0.8329,
      "step": 500
    },
    {
      "epoch": 0.6365551135033934,
      "grad_norm": 1.3112707138061523,
      "learning_rate": 0.0001644811115935736,
      "loss": 0.8443,
      "step": 510
    },
    {
      "epoch": 0.6490365863171854,
      "grad_norm": 1.2757068872451782,
      "learning_rate": 0.0001636126791141989,
      "loss": 0.8259,
      "step": 520
    },
    {
      "epoch": 0.6615180591309775,
      "grad_norm": 1.3224949836730957,
      "learning_rate": 0.00016274424663482415,
      "loss": 0.802,
      "step": 530
    },
    {
      "epoch": 0.6739995319447695,
      "grad_norm": 1.2872681617736816,
      "learning_rate": 0.0001618758141554494,
      "loss": 0.8141,
      "step": 540
    },
    {
      "epoch": 0.6864810047585616,
      "grad_norm": 1.9849917888641357,
      "learning_rate": 0.0001610073816760747,
      "loss": 0.8399,
      "step": 550
    },
    {
      "epoch": 0.6989624775723535,
      "grad_norm": 1.388184666633606,
      "learning_rate": 0.00016013894919669996,
      "loss": 0.8225,
      "step": 560
    },
    {
      "epoch": 0.7114439503861456,
      "grad_norm": 1.5182592868804932,
      "learning_rate": 0.00015927051671732524,
      "loss": 0.8136,
      "step": 570
    },
    {
      "epoch": 0.7239254231999376,
      "grad_norm": 1.3050642013549805,
      "learning_rate": 0.0001584020842379505,
      "loss": 0.8027,
      "step": 580
    },
    {
      "epoch": 0.7364068960137297,
      "grad_norm": 1.2518796920776367,
      "learning_rate": 0.00015753365175857578,
      "loss": 0.8222,
      "step": 590
    },
    {
      "epoch": 0.7488883688275216,
      "grad_norm": 1.5444879531860352,
      "learning_rate": 0.00015666521927920106,
      "loss": 0.7877,
      "step": 600
    },
    {
      "epoch": 0.7613698416413137,
      "grad_norm": 1.2311301231384277,
      "learning_rate": 0.0001557967867998263,
      "loss": 0.8019,
      "step": 610
    },
    {
      "epoch": 0.7738513144551057,
      "grad_norm": 1.3167418241500854,
      "learning_rate": 0.0001549283543204516,
      "loss": 0.808,
      "step": 620
    },
    {
      "epoch": 0.7863327872688978,
      "grad_norm": 1.39068603515625,
      "learning_rate": 0.00015405992184107687,
      "loss": 0.7965,
      "step": 630
    },
    {
      "epoch": 0.7988142600826897,
      "grad_norm": 1.50229012966156,
      "learning_rate": 0.00015319148936170213,
      "loss": 0.7765,
      "step": 640
    },
    {
      "epoch": 0.8112957328964818,
      "grad_norm": 1.339755892753601,
      "learning_rate": 0.0001523230568823274,
      "loss": 0.7764,
      "step": 650
    },
    {
      "epoch": 0.8237772057102738,
      "grad_norm": 1.2071259021759033,
      "learning_rate": 0.00015145462440295269,
      "loss": 0.7946,
      "step": 660
    },
    {
      "epoch": 0.8362586785240659,
      "grad_norm": 1.378196120262146,
      "learning_rate": 0.00015058619192357794,
      "loss": 0.8018,
      "step": 670
    },
    {
      "epoch": 0.8487401513378579,
      "grad_norm": 1.3508319854736328,
      "learning_rate": 0.00014971775944420322,
      "loss": 0.7999,
      "step": 680
    },
    {
      "epoch": 0.8612216241516499,
      "grad_norm": 1.4791686534881592,
      "learning_rate": 0.0001488493269648285,
      "loss": 0.7861,
      "step": 690
    },
    {
      "epoch": 0.8737030969654419,
      "grad_norm": 1.558600664138794,
      "learning_rate": 0.00014798089448545375,
      "loss": 0.7942,
      "step": 700
    },
    {
      "epoch": 0.886184569779234,
      "grad_norm": 1.295104742050171,
      "learning_rate": 0.00014711246200607903,
      "loss": 0.758,
      "step": 710
    },
    {
      "epoch": 0.898666042593026,
      "grad_norm": 1.4220695495605469,
      "learning_rate": 0.00014624402952670431,
      "loss": 0.7589,
      "step": 720
    },
    {
      "epoch": 0.911147515406818,
      "grad_norm": 1.5149469375610352,
      "learning_rate": 0.00014537559704732957,
      "loss": 0.7452,
      "step": 730
    },
    {
      "epoch": 0.92362898822061,
      "grad_norm": 1.400970458984375,
      "learning_rate": 0.00014450716456795485,
      "loss": 0.7528,
      "step": 740
    },
    {
      "epoch": 0.9361104610344021,
      "grad_norm": 1.415027141571045,
      "learning_rate": 0.00014363873208858013,
      "loss": 0.756,
      "step": 750
    },
    {
      "epoch": 0.9485919338481941,
      "grad_norm": 1.368098258972168,
      "learning_rate": 0.00014277029960920538,
      "loss": 0.7869,
      "step": 760
    },
    {
      "epoch": 0.9610734066619862,
      "grad_norm": 1.5073879957199097,
      "learning_rate": 0.00014190186712983066,
      "loss": 0.7478,
      "step": 770
    },
    {
      "epoch": 0.9735548794757781,
      "grad_norm": 1.4171160459518433,
      "learning_rate": 0.00014103343465045594,
      "loss": 0.7749,
      "step": 780
    },
    {
      "epoch": 0.9860363522895702,
      "grad_norm": 1.565507173538208,
      "learning_rate": 0.0001401650021710812,
      "loss": 0.7741,
      "step": 790
    },
    {
      "epoch": 0.9985178251033622,
      "grad_norm": 1.6277989149093628,
      "learning_rate": 0.00013929656969170648,
      "loss": 0.7645,
      "step": 800
    },
    {
      "epoch": 1.0099851782510336,
      "grad_norm": 1.5688570737838745,
      "learning_rate": 0.00013842813721233173,
      "loss": 0.6882,
      "step": 810
    },
    {
      "epoch": 1.0224666510648257,
      "grad_norm": 1.6793534755706787,
      "learning_rate": 0.00013755970473295704,
      "loss": 0.7066,
      "step": 820
    },
    {
      "epoch": 1.0349481238786176,
      "grad_norm": 1.4467434883117676,
      "learning_rate": 0.0001366912722535823,
      "loss": 0.6998,
      "step": 830
    },
    {
      "epoch": 1.0474295966924096,
      "grad_norm": 1.373786449432373,
      "learning_rate": 0.00013582283977420754,
      "loss": 0.7072,
      "step": 840
    },
    {
      "epoch": 1.0599110695062017,
      "grad_norm": 1.430590271949768,
      "learning_rate": 0.00013495440729483285,
      "loss": 0.723,
      "step": 850
    },
    {
      "epoch": 1.0723925423199938,
      "grad_norm": 1.6183230876922607,
      "learning_rate": 0.0001340859748154581,
      "loss": 0.7235,
      "step": 860
    },
    {
      "epoch": 1.084874015133786,
      "grad_norm": 1.465623140335083,
      "learning_rate": 0.00013321754233608336,
      "loss": 0.6913,
      "step": 870
    },
    {
      "epoch": 1.0973554879475778,
      "grad_norm": 1.5475046634674072,
      "learning_rate": 0.00013234910985670864,
      "loss": 0.7066,
      "step": 880
    },
    {
      "epoch": 1.1098369607613698,
      "grad_norm": 1.561354160308838,
      "learning_rate": 0.00013148067737733392,
      "loss": 0.7237,
      "step": 890
    },
    {
      "epoch": 1.122318433575162,
      "grad_norm": 1.5681077241897583,
      "learning_rate": 0.00013061224489795917,
      "loss": 0.725,
      "step": 900
    },
    {
      "epoch": 1.1347999063889538,
      "grad_norm": 1.6474310159683228,
      "learning_rate": 0.00012974381241858445,
      "loss": 0.6915,
      "step": 910
    },
    {
      "epoch": 1.1472813792027459,
      "grad_norm": 1.4876470565795898,
      "learning_rate": 0.00012887537993920973,
      "loss": 0.7138,
      "step": 920
    },
    {
      "epoch": 1.159762852016538,
      "grad_norm": 1.458205223083496,
      "learning_rate": 0.00012800694745983501,
      "loss": 0.7136,
      "step": 930
    },
    {
      "epoch": 1.17224432483033,
      "grad_norm": 1.567929744720459,
      "learning_rate": 0.00012713851498046027,
      "loss": 0.6949,
      "step": 940
    },
    {
      "epoch": 1.1847257976441221,
      "grad_norm": 1.5608479976654053,
      "learning_rate": 0.00012627008250108555,
      "loss": 0.6944,
      "step": 950
    },
    {
      "epoch": 1.197207270457914,
      "grad_norm": 1.6010583639144897,
      "learning_rate": 0.00012540165002171083,
      "loss": 0.6936,
      "step": 960
    },
    {
      "epoch": 1.209688743271706,
      "grad_norm": 1.6362379789352417,
      "learning_rate": 0.00012453321754233608,
      "loss": 0.674,
      "step": 970
    },
    {
      "epoch": 1.2221702160854981,
      "grad_norm": 1.6675118207931519,
      "learning_rate": 0.00012366478506296136,
      "loss": 0.7112,
      "step": 980
    },
    {
      "epoch": 1.23465168889929,
      "grad_norm": 1.508738398551941,
      "learning_rate": 0.00012279635258358664,
      "loss": 0.6707,
      "step": 990
    },
    {
      "epoch": 1.247133161713082,
      "grad_norm": 1.660474181175232,
      "learning_rate": 0.0001219279201042119,
      "loss": 0.6767,
      "step": 1000
    },
    {
      "epoch": 1.2596146345268742,
      "grad_norm": 1.566878080368042,
      "learning_rate": 0.00012105948762483716,
      "loss": 0.7085,
      "step": 1010
    },
    {
      "epoch": 1.2720961073406662,
      "grad_norm": 1.6759209632873535,
      "learning_rate": 0.00012019105514546244,
      "loss": 0.6949,
      "step": 1020
    },
    {
      "epoch": 1.2845775801544583,
      "grad_norm": 1.6062943935394287,
      "learning_rate": 0.00011932262266608771,
      "loss": 0.6692,
      "step": 1030
    },
    {
      "epoch": 1.2970590529682502,
      "grad_norm": 1.7841465473175049,
      "learning_rate": 0.000118454190186713,
      "loss": 0.6894,
      "step": 1040
    },
    {
      "epoch": 1.3095405257820423,
      "grad_norm": 1.6325006484985352,
      "learning_rate": 0.00011758575770733826,
      "loss": 0.6752,
      "step": 1050
    },
    {
      "epoch": 1.3220219985958344,
      "grad_norm": 1.7083133459091187,
      "learning_rate": 0.00011671732522796353,
      "loss": 0.6961,
      "step": 1060
    },
    {
      "epoch": 1.3345034714096262,
      "grad_norm": 1.8213753700256348,
      "learning_rate": 0.0001158488927485888,
      "loss": 0.6917,
      "step": 1070
    },
    {
      "epoch": 1.3469849442234183,
      "grad_norm": 1.6131318807601929,
      "learning_rate": 0.00011498046026921407,
      "loss": 0.6938,
      "step": 1080
    },
    {
      "epoch": 1.3594664170372104,
      "grad_norm": 1.6403082609176636,
      "learning_rate": 0.00011411202778983934,
      "loss": 0.6614,
      "step": 1090
    },
    {
      "epoch": 1.3719478898510025,
      "grad_norm": 1.5171083211898804,
      "learning_rate": 0.00011324359531046462,
      "loss": 0.685,
      "step": 1100
    },
    {
      "epoch": 1.3844293626647945,
      "grad_norm": 1.905619502067566,
      "learning_rate": 0.00011237516283108989,
      "loss": 0.688,
      "step": 1110
    },
    {
      "epoch": 1.3969108354785864,
      "grad_norm": 1.6760443449020386,
      "learning_rate": 0.00011150673035171515,
      "loss": 0.6674,
      "step": 1120
    },
    {
      "epoch": 1.4093923082923785,
      "grad_norm": 1.5658178329467773,
      "learning_rate": 0.00011063829787234043,
      "loss": 0.6703,
      "step": 1130
    },
    {
      "epoch": 1.4218737811061706,
      "grad_norm": 1.7321687936782837,
      "learning_rate": 0.0001097698653929657,
      "loss": 0.6657,
      "step": 1140
    },
    {
      "epoch": 1.4343552539199624,
      "grad_norm": 1.7155256271362305,
      "learning_rate": 0.00010890143291359098,
      "loss": 0.6696,
      "step": 1150
    },
    {
      "epoch": 1.4468367267337545,
      "grad_norm": 1.6291744709014893,
      "learning_rate": 0.00010803300043421625,
      "loss": 0.676,
      "step": 1160
    },
    {
      "epoch": 1.4593181995475466,
      "grad_norm": 1.7828680276870728,
      "learning_rate": 0.00010716456795484152,
      "loss": 0.645,
      "step": 1170
    },
    {
      "epoch": 1.4717996723613387,
      "grad_norm": 1.735348105430603,
      "learning_rate": 0.0001062961354754668,
      "loss": 0.6701,
      "step": 1180
    },
    {
      "epoch": 1.4842811451751308,
      "grad_norm": 1.7992079257965088,
      "learning_rate": 0.00010542770299609206,
      "loss": 0.6732,
      "step": 1190
    },
    {
      "epoch": 1.4967626179889226,
      "grad_norm": 1.6022883653640747,
      "learning_rate": 0.00010455927051671732,
      "loss": 0.6382,
      "step": 1200
    },
    {
      "epoch": 1.5092440908027147,
      "grad_norm": 1.7175114154815674,
      "learning_rate": 0.00010369083803734261,
      "loss": 0.6642,
      "step": 1210
    },
    {
      "epoch": 1.5217255636165068,
      "grad_norm": 1.891886591911316,
      "learning_rate": 0.00010282240555796788,
      "loss": 0.6615,
      "step": 1220
    },
    {
      "epoch": 1.5342070364302987,
      "grad_norm": 1.5952441692352295,
      "learning_rate": 0.00010195397307859313,
      "loss": 0.6448,
      "step": 1230
    },
    {
      "epoch": 1.546688509244091,
      "grad_norm": 1.963133692741394,
      "learning_rate": 0.00010108554059921842,
      "loss": 0.6443,
      "step": 1240
    },
    {
      "epoch": 1.5591699820578828,
      "grad_norm": 1.5673027038574219,
      "learning_rate": 0.00010021710811984368,
      "loss": 0.653,
      "step": 1250
    },
    {
      "epoch": 1.571651454871675,
      "grad_norm": 1.6762926578521729,
      "learning_rate": 9.934867564046896e-05,
      "loss": 0.6463,
      "step": 1260
    },
    {
      "epoch": 1.584132927685467,
      "grad_norm": 1.7857801914215088,
      "learning_rate": 9.848024316109424e-05,
      "loss": 0.6411,
      "step": 1270
    },
    {
      "epoch": 1.5966144004992588,
      "grad_norm": 1.5734535455703735,
      "learning_rate": 9.761181068171949e-05,
      "loss": 0.6488,
      "step": 1280
    },
    {
      "epoch": 1.609095873313051,
      "grad_norm": 1.7891638278961182,
      "learning_rate": 9.674337820234477e-05,
      "loss": 0.6398,
      "step": 1290
    },
    {
      "epoch": 1.621577346126843,
      "grad_norm": 1.74317467212677,
      "learning_rate": 9.587494572297004e-05,
      "loss": 0.6436,
      "step": 1300
    },
    {
      "epoch": 1.6340588189406349,
      "grad_norm": 1.8463412523269653,
      "learning_rate": 9.500651324359532e-05,
      "loss": 0.634,
      "step": 1310
    },
    {
      "epoch": 1.6465402917544272,
      "grad_norm": 1.9502944946289062,
      "learning_rate": 9.413808076422059e-05,
      "loss": 0.6386,
      "step": 1320
    },
    {
      "epoch": 1.659021764568219,
      "grad_norm": 1.863167405128479,
      "learning_rate": 9.326964828484585e-05,
      "loss": 0.6364,
      "step": 1330
    },
    {
      "epoch": 1.6715032373820111,
      "grad_norm": 1.8759270906448364,
      "learning_rate": 9.240121580547113e-05,
      "loss": 0.65,
      "step": 1340
    },
    {
      "epoch": 1.6839847101958032,
      "grad_norm": 1.6921350955963135,
      "learning_rate": 9.15327833260964e-05,
      "loss": 0.6133,
      "step": 1350
    },
    {
      "epoch": 1.696466183009595,
      "grad_norm": 1.6177538633346558,
      "learning_rate": 9.066435084672167e-05,
      "loss": 0.632,
      "step": 1360
    },
    {
      "epoch": 1.7089476558233871,
      "grad_norm": 1.778322458267212,
      "learning_rate": 8.979591836734695e-05,
      "loss": 0.6604,
      "step": 1370
    },
    {
      "epoch": 1.7214291286371792,
      "grad_norm": 1.97162926197052,
      "learning_rate": 8.892748588797222e-05,
      "loss": 0.6148,
      "step": 1380
    },
    {
      "epoch": 1.733910601450971,
      "grad_norm": 1.9667521715164185,
      "learning_rate": 8.805905340859748e-05,
      "loss": 0.6259,
      "step": 1390
    },
    {
      "epoch": 1.7463920742647634,
      "grad_norm": 1.8049348592758179,
      "learning_rate": 8.719062092922275e-05,
      "loss": 0.615,
      "step": 1400
    },
    {
      "epoch": 1.7588735470785553,
      "grad_norm": 1.8404450416564941,
      "learning_rate": 8.632218844984803e-05,
      "loss": 0.6625,
      "step": 1410
    },
    {
      "epoch": 1.7713550198923473,
      "grad_norm": 1.844909429550171,
      "learning_rate": 8.545375597047331e-05,
      "loss": 0.6295,
      "step": 1420
    },
    {
      "epoch": 1.7838364927061394,
      "grad_norm": 1.7813318967819214,
      "learning_rate": 8.458532349109856e-05,
      "loss": 0.6368,
      "step": 1430
    },
    {
      "epoch": 1.7963179655199313,
      "grad_norm": 1.9008729457855225,
      "learning_rate": 8.371689101172384e-05,
      "loss": 0.6109,
      "step": 1440
    },
    {
      "epoch": 1.8087994383337234,
      "grad_norm": 1.6461129188537598,
      "learning_rate": 8.284845853234911e-05,
      "loss": 0.6104,
      "step": 1450
    },
    {
      "epoch": 1.8212809111475154,
      "grad_norm": 1.8028554916381836,
      "learning_rate": 8.198002605297439e-05,
      "loss": 0.6316,
      "step": 1460
    },
    {
      "epoch": 1.8337623839613073,
      "grad_norm": 1.974220871925354,
      "learning_rate": 8.111159357359966e-05,
      "loss": 0.6222,
      "step": 1470
    },
    {
      "epoch": 1.8462438567750996,
      "grad_norm": 1.8353028297424316,
      "learning_rate": 8.024316109422493e-05,
      "loss": 0.6222,
      "step": 1480
    },
    {
      "epoch": 1.8587253295888915,
      "grad_norm": 2.039625406265259,
      "learning_rate": 7.93747286148502e-05,
      "loss": 0.6088,
      "step": 1490
    },
    {
      "epoch": 1.8712068024026836,
      "grad_norm": 1.7221275568008423,
      "learning_rate": 7.850629613547546e-05,
      "loss": 0.6263,
      "step": 1500
    },
    {
      "epoch": 1.8836882752164756,
      "grad_norm": 1.9458602666854858,
      "learning_rate": 7.763786365610074e-05,
      "loss": 0.6324,
      "step": 1510
    },
    {
      "epoch": 1.8961697480302675,
      "grad_norm": 1.710453987121582,
      "learning_rate": 7.676943117672602e-05,
      "loss": 0.607,
      "step": 1520
    },
    {
      "epoch": 1.9086512208440596,
      "grad_norm": 1.7088590860366821,
      "learning_rate": 7.590099869735129e-05,
      "loss": 0.6016,
      "step": 1530
    },
    {
      "epoch": 1.9211326936578517,
      "grad_norm": 1.9098882675170898,
      "learning_rate": 7.503256621797655e-05,
      "loss": 0.6298,
      "step": 1540
    },
    {
      "epoch": 1.9336141664716435,
      "grad_norm": 1.8497443199157715,
      "learning_rate": 7.416413373860182e-05,
      "loss": 0.6267,
      "step": 1550
    },
    {
      "epoch": 1.9460956392854358,
      "grad_norm": 2.1784982681274414,
      "learning_rate": 7.32957012592271e-05,
      "loss": 0.5969,
      "step": 1560
    },
    {
      "epoch": 1.9585771120992277,
      "grad_norm": 1.8442587852478027,
      "learning_rate": 7.242726877985238e-05,
      "loss": 0.6305,
      "step": 1570
    },
    {
      "epoch": 1.9710585849130198,
      "grad_norm": 2.0402562618255615,
      "learning_rate": 7.155883630047764e-05,
      "loss": 0.6271,
      "step": 1580
    },
    {
      "epoch": 1.9835400577268119,
      "grad_norm": 1.7165536880493164,
      "learning_rate": 7.069040382110292e-05,
      "loss": 0.6247,
      "step": 1590
    },
    {
      "epoch": 1.9960215305406037,
      "grad_norm": 1.7710822820663452,
      "learning_rate": 6.982197134172818e-05,
      "loss": 0.6019,
      "step": 1600
    },
    {
      "epoch": 2.0074888836882754,
      "grad_norm": 1.9456562995910645,
      "learning_rate": 6.895353886235345e-05,
      "loss": 0.5951,
      "step": 1610
    },
    {
      "epoch": 2.0199703565020672,
      "grad_norm": 1.995944619178772,
      "learning_rate": 6.808510638297873e-05,
      "loss": 0.5915,
      "step": 1620
    },
    {
      "epoch": 2.032451829315859,
      "grad_norm": 1.92649245262146,
      "learning_rate": 6.7216673903604e-05,
      "loss": 0.5694,
      "step": 1630
    },
    {
      "epoch": 2.0449333021296514,
      "grad_norm": 1.8890875577926636,
      "learning_rate": 6.634824142422928e-05,
      "loss": 0.5608,
      "step": 1640
    },
    {
      "epoch": 2.0574147749434433,
      "grad_norm": 2.223148822784424,
      "learning_rate": 6.547980894485453e-05,
      "loss": 0.5836,
      "step": 1650
    },
    {
      "epoch": 2.069896247757235,
      "grad_norm": 1.7985013723373413,
      "learning_rate": 6.461137646547981e-05,
      "loss": 0.5658,
      "step": 1660
    },
    {
      "epoch": 2.0823777205710274,
      "grad_norm": 2.1132500171661377,
      "learning_rate": 6.374294398610509e-05,
      "loss": 0.5742,
      "step": 1670
    },
    {
      "epoch": 2.0948591933848193,
      "grad_norm": 2.087831974029541,
      "learning_rate": 6.287451150673034e-05,
      "loss": 0.5726,
      "step": 1680
    },
    {
      "epoch": 2.1073406661986116,
      "grad_norm": 2.320472478866577,
      "learning_rate": 6.200607902735563e-05,
      "loss": 0.5741,
      "step": 1690
    },
    {
      "epoch": 2.1198221390124035,
      "grad_norm": 2.161288022994995,
      "learning_rate": 6.113764654798089e-05,
      "loss": 0.5675,
      "step": 1700
    },
    {
      "epoch": 2.1323036118261953,
      "grad_norm": 1.9986572265625,
      "learning_rate": 6.026921406860617e-05,
      "loss": 0.5683,
      "step": 1710
    },
    {
      "epoch": 2.1447850846399876,
      "grad_norm": 2.082509994506836,
      "learning_rate": 5.940078158923143e-05,
      "loss": 0.574,
      "step": 1720
    },
    {
      "epoch": 2.1572665574537795,
      "grad_norm": 2.1017940044403076,
      "learning_rate": 5.853234910985671e-05,
      "loss": 0.555,
      "step": 1730
    },
    {
      "epoch": 2.169748030267572,
      "grad_norm": 1.9865267276763916,
      "learning_rate": 5.766391663048199e-05,
      "loss": 0.5505,
      "step": 1740
    },
    {
      "epoch": 2.1822295030813637,
      "grad_norm": 2.0745229721069336,
      "learning_rate": 5.679548415110726e-05,
      "loss": 0.5415,
      "step": 1750
    },
    {
      "epoch": 2.1947109758951555,
      "grad_norm": 2.3154797554016113,
      "learning_rate": 5.592705167173252e-05,
      "loss": 0.5648,
      "step": 1760
    },
    {
      "epoch": 2.207192448708948,
      "grad_norm": 2.1479439735412598,
      "learning_rate": 5.5058619192357795e-05,
      "loss": 0.5645,
      "step": 1770
    },
    {
      "epoch": 2.2196739215227397,
      "grad_norm": 1.9060879945755005,
      "learning_rate": 5.419018671298307e-05,
      "loss": 0.5576,
      "step": 1780
    },
    {
      "epoch": 2.2321553943365315,
      "grad_norm": 2.067377805709839,
      "learning_rate": 5.3321754233608335e-05,
      "loss": 0.5639,
      "step": 1790
    },
    {
      "epoch": 2.244636867150324,
      "grad_norm": 1.9655014276504517,
      "learning_rate": 5.245332175423361e-05,
      "loss": 0.5589,
      "step": 1800
    },
    {
      "epoch": 2.2571183399641157,
      "grad_norm": 1.8807717561721802,
      "learning_rate": 5.158488927485888e-05,
      "loss": 0.5547,
      "step": 1810
    },
    {
      "epoch": 2.2695998127779076,
      "grad_norm": 1.9387824535369873,
      "learning_rate": 5.0716456795484156e-05,
      "loss": 0.5401,
      "step": 1820
    },
    {
      "epoch": 2.2820812855917,
      "grad_norm": 2.0101845264434814,
      "learning_rate": 4.984802431610942e-05,
      "loss": 0.5483,
      "step": 1830
    },
    {
      "epoch": 2.2945627584054917,
      "grad_norm": 2.0917251110076904,
      "learning_rate": 4.89795918367347e-05,
      "loss": 0.5554,
      "step": 1840
    },
    {
      "epoch": 2.307044231219284,
      "grad_norm": 2.0332305431365967,
      "learning_rate": 4.811115935735997e-05,
      "loss": 0.5582,
      "step": 1850
    },
    {
      "epoch": 2.319525704033076,
      "grad_norm": 2.321094036102295,
      "learning_rate": 4.724272687798524e-05,
      "loss": 0.5537,
      "step": 1860
    },
    {
      "epoch": 2.3320071768468678,
      "grad_norm": 2.061947822570801,
      "learning_rate": 4.637429439861051e-05,
      "loss": 0.5467,
      "step": 1870
    },
    {
      "epoch": 2.34448864966066,
      "grad_norm": 1.954003095626831,
      "learning_rate": 4.5505861919235785e-05,
      "loss": 0.5656,
      "step": 1880
    },
    {
      "epoch": 2.356970122474452,
      "grad_norm": 1.966854453086853,
      "learning_rate": 4.463742943986105e-05,
      "loss": 0.5458,
      "step": 1890
    },
    {
      "epoch": 2.3694515952882442,
      "grad_norm": 2.0611960887908936,
      "learning_rate": 4.3768996960486325e-05,
      "loss": 0.5436,
      "step": 1900
    },
    {
      "epoch": 2.381933068102036,
      "grad_norm": 2.2569832801818848,
      "learning_rate": 4.290056448111159e-05,
      "loss": 0.5611,
      "step": 1910
    },
    {
      "epoch": 2.394414540915828,
      "grad_norm": 2.0712406635284424,
      "learning_rate": 4.2032132001736866e-05,
      "loss": 0.5454,
      "step": 1920
    },
    {
      "epoch": 2.4068960137296203,
      "grad_norm": 2.3480560779571533,
      "learning_rate": 4.116369952236214e-05,
      "loss": 0.5526,
      "step": 1930
    },
    {
      "epoch": 2.419377486543412,
      "grad_norm": 2.0375661849975586,
      "learning_rate": 4.029526704298741e-05,
      "loss": 0.5341,
      "step": 1940
    },
    {
      "epoch": 2.431858959357204,
      "grad_norm": 2.0992062091827393,
      "learning_rate": 3.942683456361268e-05,
      "loss": 0.5347,
      "step": 1950
    },
    {
      "epoch": 2.4443404321709963,
      "grad_norm": 2.043811798095703,
      "learning_rate": 3.8558402084237954e-05,
      "loss": 0.5774,
      "step": 1960
    },
    {
      "epoch": 2.456821904984788,
      "grad_norm": 2.0395705699920654,
      "learning_rate": 3.768996960486322e-05,
      "loss": 0.5535,
      "step": 1970
    },
    {
      "epoch": 2.46930337779858,
      "grad_norm": 2.062056064605713,
      "learning_rate": 3.6821537125488495e-05,
      "loss": 0.555,
      "step": 1980
    },
    {
      "epoch": 2.4817848506123723,
      "grad_norm": 2.341569185256958,
      "learning_rate": 3.595310464611377e-05,
      "loss": 0.5573,
      "step": 1990
    },
    {
      "epoch": 2.494266323426164,
      "grad_norm": 2.274247169494629,
      "learning_rate": 3.5084672166739035e-05,
      "loss": 0.5403,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2403,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.24654775733846e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
