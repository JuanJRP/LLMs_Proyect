# LLaMA LoRA Fine-tuning Project

Este proyecto implementa el fine-tuning de LLaMA-2-7B usando LoRA (Low-Rank Adaptation) para crear un modelo especializado en productos de consumo masivo, con integraci√≥n a un bot de Telegram.

## üìã Tabla de Contenidos

- [Caracter√≠sticas](#caracter√≠sticas)
- [Requisitos del Sistema](#requisitos-del-sistema)
- [Instalaci√≥n](#instalaci√≥n)
- [Configuraci√≥n](#configuraci√≥n)
- [Uso](#uso)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Detalles T√©cnicos](#detalles-t√©cnicos)
- [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
- [Contribuciones](#contribuciones)

## üöÄ Caracter√≠sticas

- **Fine-tuning eficiente**: Utiliza LoRA para entrenar solo una fracci√≥n de los par√°metros del modelo
- **Optimizaci√≥n de memoria**: Implementa cuantizaci√≥n de 4-bit para reducir el uso de VRAM
- **Dataset personalizado**: Entrena con datos de productos de consumo masivo de Kaggle
- **Bot de Telegram**: Interfaz conversacional para interactuar con el modelo entrenado
- **Monitoreo**: Integraci√≥n con TensorBoard para seguimiento del entrenamiento

## üíª Requisitos del Sistema

### Hardware M√≠nimo
- **GPU**: NVIDIA con al menos 8GB VRAM (recomendado 16GB+)
- **RAM**: 16GB m√≠nimo (recomendado 32GB+)
- **Almacenamiento**: 50GB de espacio libre

### Software
- Python 3.8+
- CUDA 11.8+ (compatible con PyTorch 2.0+)
- Git

## üì¶ Instalaci√≥n

### 1. Clonar el repositorio
```bash
git clone https://github.com/JuanJRP/LLaMa_LoRA_Project.git
cd LLaMa-LoRA-Project
```

### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Configurar Hugging Face
```bash
huggingface-cli login
```
Necesitar√°s un token de Hugging Face con acceso al modelo LLaMA-2.

## ‚öôÔ∏è Configuraci√≥n

### 1. Acceso al modelo LLaMA-2
- Solicita acceso a LLaMA-2 en [Meta AI](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
- Acepta los t√©rminos en [Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

### 2. Token de Telegram Bot
- Crea un bot con [@BotFather](https://t.me/botfather)
- Reemplaza `TELEGRAM_TOKEN` en `Test_Model.py` con tu token
```python
TELEGRAM_TOKEN = "tu_token_aqui"
```

### 3. Configuraci√≥n de paths
Actualiza las rutas en los scripts seg√∫n tu estructura de directorios:
```bash
# En run.sh y runBot.sh
python /ruta/a/tu/proyecto/LLaMa_LoRA.py
```

## üéØ Uso

### Entrenamiento del Modelo

#### Opci√≥n 1: Script directo
```bash
python LLaMa\LLaMa_LoRA.py
```

#### Opci√≥n 2: Script automatizado
```bash
chmod +x run.sh
./run.sh
```

### Monitoreo del Entrenamiento
```bash
tensorboard --logdir=./logs
```
Accede a `http://localhost:6006` para ver m√©tricas en tiempo real.

### Ejecutar el Bot de Telegram

#### Opci√≥n 1: Script directo
```bash
python Test_Model.py
```

#### Opci√≥n 2: Script automatizado
```bash
chmod +x runBot.sh
./runBot.sh
```

### Interacci√≥n con el Bot
1. Busca tu bot en Telegram
2. Env√≠a cualquier mensaje
3. El bot responder√° con informaci√≥n contextualizada

## üìÅ Estructura del Proyecto

```
llama-lora-project/
‚îú‚îÄ‚îÄ LLaMa_LoRA.py          # Script principal de entrenamiento
‚îú‚îÄ‚îÄ testmodel.py           # Bot de Telegram con modelo entrenado
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias del proyecto
‚îú‚îÄ‚îÄ run.sh                # Script de entrenamiento automatizado
‚îú‚îÄ‚îÄ runBot.sh             # Script del bot automatizado
‚îú‚îÄ‚îÄ README.md             # Documentaci√≥n del proyecto
‚îú‚îÄ‚îÄ llama2-lora-spanish-final/  # Modelo entrenado (generado)
‚îú‚îÄ‚îÄ logs/                 # Logs de TensorBoard (generado)
‚îî‚îÄ‚îÄ output - Kaggle.xlsx  # Dataset descargado (generado)
```

## üîß Detalles T√©cnicos

### Configuraci√≥n de LoRA
```python
LoraConfig(
    r=6,                    # Rango de matrices de bajo rango
    lora_alpha=16,          # Factor de escala
    lora_dropout=0.05,      # Dropout para regularizaci√≥n
    target_modules=["q_proj", "v_proj"]  # Capas objetivo
)
```

### Optimizaciones de Memoria
- **Cuantizaci√≥n 4-bit**: Reduce uso de memoria en ~75%
- **Gradient Checkpointing**: Intercambia memoria por c√≥mputo
- **Batch Size Peque√±o**: 2 ejemplos por dispositivo
- **Gradient Accumulation**: 16 pasos para batch efectivo de 32

### Par√°metros de Entrenamiento
```python
TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
    learning_rate=2e-4,
    fp16=True
)
```

### Dataset
- **Fuente**: Kaggle - Productos de Consumo Masivo
- **Tama√±o**: 8,000 productos (limitado para pruebas)
- **Formato**: Texto estructurado con nombre, subcategor√≠a, tags y precio
- **Divisi√≥n**: 90% entrenamiento, 10% evaluaci√≥n

## üõ†Ô∏è Soluci√≥n de Problemas

### Error de Memoria CUDA
```
RuntimeError: CUDA out of memory
```
**Soluci√≥n:**
- Reduce `per_device_train_batch_size` a 1
- Aumenta `gradient_accumulation_steps`
- Reduce `max_length` en tokenizaci√≥n

### Token de Hugging Face
```
401 Client Error: Unauthorized
```
**Soluci√≥n:**
```bash
huggingface-cli login --token your_token_here
```

### Error de Permisos en Scripts
```bash
chmod +x run.sh runBot.sh
```

### Bot de Telegram no responde
1. Verifica que el token sea correcto
2. Confirma que el modelo est√© cargado correctamente
3. Revisa los logs de errores en consola

### Instalaci√≥n de BitsAndBytesConfig
Si encuentras errores con `bitsandbytes`:
```bash
# Para sistemas con CUDA
pip install bitsandbytes
# Para sistemas sin CUDA
pip install bitsandbytes-cpu
```

## üìä M√©tricas y Evaluaci√≥n

### Par√°metros Entrenables
El modelo muestra qu√© porcentaje de par√°metros se entrena:
```
trainable params: 1,769,472 || all params: 6,740,781,056 || trainable%: 0.02625
```

### M√©tricas de TensorBoard
- Loss de entrenamiento
- Learning rate schedule
- Gradient norms
- Tiempo por step


## üìù Notas Importantes

### Consideraciones de Licencia
- LLaMA-2 requiere licencia de Meta AI
- Uso comercial limitado seg√∫n t√©rminos de Meta
- Revisar t√©rminos antes de producci√≥n

### Rendimiento
- Tiempo de entrenamiento: ~2-4 horas en GPU moderna
- Inferencia: ~1-3 segundos por respuesta
- Tama√±o del modelo: ~13GB (modelo base + adaptadores LoRA)


## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver `LICENSE` para m√°s detalles.

## üìû Soporte

Para preguntas o problemas:
- Abre un issue en GitHub
- Consulta la documentaci√≥n de [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- Revisa la documentaci√≥n de [PEFT](https://huggingface.co/docs/peft)

---

**‚ö†Ô∏è Advertencia**: Este es un proyecto educativo. Para uso en producci√≥n, considera aspectos adicionales de seguridad, escalabilidad y cumplimiento normativo.