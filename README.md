# LLaMA LoRA Fine-tuning Project

Este proyecto implementa el ajuste fino (fine-tuning) de modelos LLaMA-2 en espaÃ±ol utilizando la tÃ©cnica LoRA (Low-Rank Adaptation) y la librerÃ­a PEFT. El objetivo es facilitar el entrenamiento, evaluaciÃ³n y despliegue de modelos de lenguaje adaptados a tareas especÃ­ficas en espaÃ±ol.

## Estructura del Proyecto

```
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Commands.txt
â”œâ”€â”€ optimized/
â”‚   â”œâ”€â”€ LLaMa/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ nohup/
â”‚   â”‚   â”œâ”€â”€ llama2-lora-spanish/
â”‚   â”‚   â”œâ”€â”€ llama2-lora-spanish-final/
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ run.sh
â”‚   â”‚   â””â”€â”€ runBot.sh
â”‚   â””â”€â”€ source/
â”œâ”€â”€ working/
â”‚   â”œâ”€â”€ LLaMa/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ llama2-lora-spanish/
â”‚   â”œâ”€â”€ nohup/
â”‚   â”‚   â”œâ”€â”€ Full.sh
â”‚   â”‚   â”œâ”€â”€ run.sh
â”‚   â”‚   â””â”€â”€ runBot.sh
â”‚   â””â”€â”€ source/
```

## ğŸš€ CaracterÃ­sticas
- **optimized/**: Scripts y modelos optimizados para entrenamiento y despliegue.
- **working/**: Ãrea de trabajo principal para experimentaciÃ³n, logs y checkpoints.
- **model/**: Checkpoints y resultados de modelos entrenados.
- **nohup/**: Scripts de ejecuciÃ³n en segundo plano para entrenamiento y bots.
- **logs/**: Salidas y registros de entrenamiento y bots.
- **source/**: Scripts de prueba y utilidades.


## ğŸ’» Requisitos del Sistema

### Hardware MÃ­nimo

- **GPU**: NVIDIA con al menos 8GB VRAM (recomendado 16GB+)
- **RAM**: 16GB mÃ­nimo (recomendado 32GB+)
- **Almacenamiento**: 50GB de espacio libre


### Software

- Python 3.8+
- CUDA 11.8+ (compatible con PyTorch 2.0+)w
- Git


## InstalaciÃ³n

1. Clona el repositorio.
```bash
git clone https://github.com/JuanJRP/LLaMa_LoRA_Project.git
cd LLaMa-LoRA-Project
```
### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows
```
### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```
### 4. Configurar Hugging Face
```bash
huggingface-cli login
```
NecesitarÃ¡s un token de Hugging Face con acceso al modelo LLaMA-2.

## âš™ï¸ ConfiguraciÃ³n

### 1. Acceso al modelo LLaMA-2
- Solicita acceso a LLaMA-2 en [Meta AI](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
- Acepta los tÃ©rminos en [Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
### 2. Token de Telegram Bot
- Crea un bot con [@BotFather](https://t.me/botfather)
- Reemplaza `TELEGRAM_TOKEN` en `Test_Model.py` con tu token
```python
TELEGRAM_TOKEN = "tu_token_aqui"
```
### 3. ConfiguraciÃ³n de paths
Actualiza las rutas en los scripts segÃºn tu estructura de directorios:
```bash
# En run.sh y runBot.sh
python /ruta/a/tu/proyecto/LLaMa_LoRA.py
```

## Entrenamiento y EjecuciÃ³n
Para entrenar el modelo y ejecutar el bot, utiliza los scripts en `working/nohup/`:
```sh
bash working/nohup/Full.sh
```
Esto ejecutarÃ¡ el entrenamiento y el bot en segundo plano, guardando los logs en `working/logs/`.

## ğŸ”§ Detalles TÃ©cnicos

### ConfiguraciÃ³n de LoRA

```python
LoraConfig(
    r=6,                    # Rango de matrices de bajo rango
    lora_alpha=16,          # Factor de escala
    lora_dropout=0.05,      # Dropout para regularizaciÃ³n
    target_modules=["q_proj", "v_proj"]  # Capas objetivo
)
```

### Optimizaciones de Memoria
- **CuantizaciÃ³n 4-bit**: Reduce uso de memoria en ~75%
- **Gradient Checkpointing**: Intercambia memoria por cÃ³mputo
- **Batch Size PequeÃ±o**: 2 ejemplos por dispositivo
- **Gradient Accumulation**: 16 pasos para batch efectivo de 32

### ParÃ¡metros de Entrenamiento
```python
TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
    learning_rate=2e-4,
    fp16=True
)
```

### Dataset

- **Fuente**: Kaggle - Productos de Consumo Masivo
- **TamaÃ±o**: 8,000 productos (limitado para pruebas)
- **Formato**: Texto estructurado con nombre, subcategorÃ­a, tags y precio
- **DivisiÃ³n**: 90% entrenamiento, 10% evaluaciÃ³n

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Error de Memoria CUDA
```
RuntimeError: CUDA out of memory
```

**SoluciÃ³n:**
- Reduce `per_device_train_batch_size` a 1
- Aumenta `gradient_accumulation_steps`
- Reduce `max_length` en tokenizaciÃ³n

### Token de Hugging Face
```
401 Client Error: Unauthorized
```

**SoluciÃ³n:**
```bash
huggingface-cli login --token your_token_here
```
### Error de Permisos en Scripts
```bash
chmod +x run.sh runBot.sh

```

### Bot de Telegram no responde
1. Verifica que el token sea correcto
2. Confirma que el modelo estÃ© cargado correctamente
3. Revisa los logs de errores en consola

### InstalaciÃ³n de BitsAndBytesConfig
Si encuentras errores con `bitsandbytes`:
```bash
# Para sistemas con CUDA
pip install bitsandbytes
# Para sistemas sin CUDA
pip install bitsandbytes-cpu
```
## ğŸ“Š MÃ©tricas y EvaluaciÃ³n

### ParÃ¡metros Entrenables
El modelo muestra quÃ© porcentaje de parÃ¡metros se entrena:
```
trainable params: 1,769,472 || all params: 6,740,781,056 || trainable%: 0.02625
```

### MÃ©tricas de TensorBoard
- Loss de entrenamiento
- Learning rate schedule
- Gradient norms
- Tiempo por step

## ğŸ“ Notas Importantes

### Consideraciones de Licencia
- LLaMA-2 requiere licencia de Meta AI
- Uso comercial limitado segÃºn tÃ©rminos de Meta
- Revisar tÃ©rminos antes de producciÃ³n

### Rendimiento
- Tiempo de entrenamiento: ~2-4 horas en GPU moderna
- Inferencia: ~1-3 segundos por respuesta
- TamaÃ±o del modelo: ~13GB (modelo base + adaptadores LoRA)

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia
Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ“ Soporte

Para preguntas o problemas:
- Abre un issue en GitHub
- Consulta la documentaciÃ³n de [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- Revisa la documentaciÃ³n de [PEFT](https://huggingface.co/docs/peft)

---

**âš ï¸ Advertencia**: Este es un proyecto educativo. Para uso en producciÃ³n, considera aspectos adicionales de seguridad, escalabilidad y cumplimiento normativo.